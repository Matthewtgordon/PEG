#
# PEG/APEG Consolidated CI Pipeline
#
# This workflow consolidates peg-ci, apeg-ci, and test-matrix into a single
# efficient pipeline with:
# - Security scanning (pip-audit + bandit SAST)
# - Adaptive quality thresholds
# - Coverage ratcheting
# - Self-learning feedback integration
#
name: CI Main

on:
  push:
    branches:
      - main
      - 'claude/**'
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      skip_security:
        description: 'Skip security scans'
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.11'
  APEG_TEST_MODE: 'true'
  APEG_USE_LLM_SCORING: 'false'

jobs:
  # =================================================================
  # Phase 1: Security Scanning (pip-audit + bandit SAST)
  # =================================================================
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.skip_security != 'true' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit bandit[toml] safety

      - name: Run pip-audit (dependency vulnerabilities)
        id: pip_audit
        continue-on-error: true
        run: |
          echo "## Dependency Vulnerability Scan" >> $GITHUB_STEP_SUMMARY
          pip-audit -r requirements.txt --format json --output audit-results.json || true
          pip-audit -r requirements.txt --format markdown >> $GITHUB_STEP_SUMMARY || true

          # Check for critical vulnerabilities
          if pip-audit -r requirements.txt --strict 2>/dev/null; then
            echo "audit_passed=true" >> $GITHUB_OUTPUT
          else
            echo "audit_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Run Bandit SAST (code security)
        id: bandit
        continue-on-error: true
        run: |
          echo "## Static Application Security Testing (SAST)" >> $GITHUB_STEP_SUMMARY

          # Run bandit on all Python code
          bandit -r src/ -f json -o bandit-results.json || true
          bandit -r src/ -f markdown >> $GITHUB_STEP_SUMMARY || true

          # Check severity thresholds
          CRITICAL=$(cat bandit-results.json | python -c "import sys,json; d=json.load(sys.stdin); print(sum(1 for i in d.get('results',[]) if i.get('issue_severity')=='HIGH'))" 2>/dev/null || echo "0")
          echo "critical_issues=$CRITICAL" >> $GITHUB_OUTPUT

          if [ "$CRITICAL" -eq "0" ]; then
            echo "bandit_passed=true" >> $GITHUB_OUTPUT
          else
            echo "bandit_passed=false" >> $GITHUB_OUTPUT
            echo "::warning::Found $CRITICAL high-severity security issues"
          fi

      - name: Generate SBOM (Software Bill of Materials)
        run: |
          pip install pip-licenses
          pip-licenses --format=json --output-file=sbom.json
          pip-licenses --format=markdown >> $GITHUB_STEP_SUMMARY
          echo "## SBOM Generated" >> $GITHUB_STEP_SUMMARY

      - name: Upload security artifacts
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            audit-results.json
            bandit-results.json
            sbom.json

      - name: Security gate check
        run: |
          # Load thresholds from quality-thresholds.json
          MAX_HIGH=$(cat .github/quality-thresholds.json | python -c "import sys,json; print(json.load(sys.stdin).get('security',{}).get('max_high_severity_issues', 5))" 2>/dev/null || echo "5")
          ALLOW_CRITICAL=$(cat .github/quality-thresholds.json | python -c "import sys,json; print(str(json.load(sys.stdin).get('security',{}).get('allow_critical_vulnerabilities', False)).lower())" 2>/dev/null || echo "false")

          # Check audit results
          if [ "${{ steps.pip_audit.outputs.audit_passed }}" != "true" ] && [ "$ALLOW_CRITICAL" != "true" ]; then
            echo "::error::Dependency vulnerabilities detected and critical vulns not allowed"
            exit 1
          fi

          # Check bandit results
          CRITICAL="${{ steps.bandit.outputs.critical_issues }}"
          if [ "$CRITICAL" -gt "$MAX_HIGH" ]; then
            echo "::error::Too many high-severity security issues: $CRITICAL > $MAX_HIGH"
            exit 1
          fi

          echo "✅ Security checks passed"

  # =================================================================
  # Phase 2: Validation & Structure Check
  # =================================================================
  validate:
    name: Validate Structure
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Validate repository structure
        run: python validate_repo.py

      - name: Validate APEG package structure
        run: |
          echo "Validating APEG imports..."
          python -c "from apeg_core import APEGOrchestrator; print('✓ APEGOrchestrator OK')"
          python -c "from apeg_core.decision import choose_macro, detect_loop; print('✓ Decision engine OK')"
          python -c "from apeg_core.agents import ShopifyAgent, EtsyAgent; print('✓ Agents OK')"
          python -c "from apeg_core.scoring.evaluator import Evaluator; print('✓ Evaluator OK')"
          python -c "from apeg_core.logging.logbook_adapter import LogbookAdapter; print('✓ LogbookAdapter OK')"
          python -c "from apeg_core.memory.memory_store import MemoryStore; print('✓ MemoryStore OK')"

      - name: Validate JSON configurations
        run: |
          python -c "import json; json.load(open('SessionConfig.json')); print('✓ SessionConfig.json valid')"
          python -c "import json; json.load(open('WorkflowGraph.json')); print('✓ WorkflowGraph.json valid')"
          python -c "import json; json.load(open('Knowledge.json')); print('✓ Knowledge.json valid')"
          python -c "import json; json.load(open('PromptScoreModel.json')); print('✓ PromptScoreModel.json valid')"
          python -c "import json; json.load(open('.github/quality-thresholds.json')); print('✓ quality-thresholds.json valid')"

  # =================================================================
  # Phase 3: Code Quality
  # =================================================================
  quality:
    name: Code Quality
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run Ruff linter
        id: ruff
        continue-on-error: true
        run: |
          ruff check src/apeg_core/ --output-format=github 2>&1 | tee ruff-output.txt || true
          ISSUES=$(grep -c "::" ruff-output.txt 2>/dev/null || echo "0")
          echo "ruff_issues=$ISSUES" >> $GITHUB_OUTPUT

      - name: Run MyPy type checker
        continue-on-error: true
        run: mypy src/apeg_core/ --ignore-missing-imports 2>&1 || true

      - name: Check code formatting
        continue-on-error: true
        run: black --check src/apeg_core/ 2>&1 || echo "::warning::Code formatting issues found"

      - name: Run scoring with real metrics
        run: |
          python run_scoring.py --model PromptScoreModel.json --input README.md --out score.json
          cat score.json >> $GITHUB_STEP_SUMMARY

      - name: Upload quality artifacts
        uses: actions/upload-artifact@v4
        with:
          name: quality-reports
          path: |
            score.json
            ruff-output.txt

  # =================================================================
  # Phase 4: Test Suite with Coverage
  # =================================================================
  test:
    name: Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    needs: [validate]

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov pytest-asyncio

      - name: Create test environment
        run: |
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env
          echo "GEMINI_API_KEY=${{ secrets.GOOGLE_API_KEY }}" >> .env
          echo "GITHUB_PAT=${{ secrets.GITHUB_TOKEN }}" >> .env
          echo "APEG_TEST_MODE=true" >> .env

      - name: Run tests with coverage
        run: |
          pytest tests/ \
            -v \
            --tb=short \
            --junitxml=test-results/results-${{ matrix.python-version }}.xml \
            --cov=src/apeg_core \
            --cov-report=xml:coverage-${{ matrix.python-version }}.xml \
            --cov-report=term \
            --cov-report=html:coverage-html
        env:
          PYTHONIOENCODING: utf-8
          APEG_TEST_MODE: 'true'

      - name: Adaptive coverage threshold check
        if: matrix.python-version == '3.11'
        run: |
          # Get current coverage
          COVERAGE=$(python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('coverage-3.11.xml')
          print(float(tree.getroot().attrib['line-rate']) * 100)
          ")

          # Load adaptive threshold
          THRESHOLD=$(python -c "
          import json
          try:
              with open('.github/quality-thresholds.json') as f:
                  data = json.load(f)
              # Use minimum, or ratchet up from previous
              minimum = data.get('coverage', {}).get('minimum', 60)
              print(minimum)
          except:
              print(60)
          ")

          echo "Coverage: $COVERAGE%, Threshold: $THRESHOLD%"

          if (( $(echo "$COVERAGE < $THRESHOLD" | bc -l) )); then
            echo "::error::Coverage $COVERAGE% is below threshold $THRESHOLD%"
            exit 1
          fi

          echo "✅ Coverage check passed: $COVERAGE% >= $THRESHOLD%"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test-results/
            coverage-${{ matrix.python-version }}.xml
            coverage-html/

      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.11'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage-3.11.xml
          flags: apeg
          name: apeg-coverage

  # =================================================================
  # Phase 5: Integration Tests
  # =================================================================
  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [test, quality, security]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Create environment file
        run: |
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env
          echo "GEMINI_API_KEY=${{ secrets.GOOGLE_API_KEY }}" >> .env
          echo "APEG_TEST_MODE=true" >> .env

      - name: Test end-to-end workflow
        env:
          APEG_TEST_MODE: 'true'
        run: |
          python -c "
          from apeg_core import APEGOrchestrator
          import json

          # Load configs
          with open('SessionConfig.json') as f:
              session_config = json.load(f)
          with open('WorkflowGraph.json') as f:
              workflow_graph = json.load(f)

          # Initialize orchestrator
          orch = APEGOrchestrator(session_config, workflow_graph)
          print('✓ Orchestrator initialized')

          # Test agent registry
          from apeg_core.agents import list_agents, get_agent
          agents = list_agents()
          print(f'✓ Registered agents: {agents}')

          # Test evaluator with real metrics
          from apeg_core.scoring.evaluator import Evaluator
          evaluator = Evaluator()
          result = evaluator.evaluate('Test output for integration')
          print(f'✓ Evaluator test: score={result.score:.2f}')

          # Test real metrics calculator
          try:
              from apeg_core.scoring.real_metrics import RealMetricsCalculator
              calc = RealMetricsCalculator()
              print('✓ RealMetricsCalculator available')
          except ImportError:
              print('⚠ RealMetricsCalculator not available')

          # Test logbook
          from apeg_core.logging.logbook_adapter import LogbookAdapter
          logger = LogbookAdapter(test_mode=True)
          logger.log_info('Integration test complete')
          print('✓ All integration tests passed')
          "

  # =================================================================
  # Phase 6: CI Feedback Loop (records results for bandit learning)
  # =================================================================
  feedback:
    name: Record CI Feedback
    runs-on: ubuntu-latest
    needs: [test, integration]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Download test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: all-test-results
          merge-multiple: true
        continue-on-error: true

      - name: Record CI results for bandit feedback
        run: |
          python -c "
          import json
          import os
          from datetime import datetime
          from pathlib import Path

          # Collect CI results
          ci_result = {
              'timestamp': datetime.now().isoformat(),
              'run_id': os.environ.get('GITHUB_RUN_ID', 'unknown'),
              'sha': os.environ.get('GITHUB_SHA', 'unknown'),
              'ref': os.environ.get('GITHUB_REF', 'unknown'),
              'test_status': '${{ needs.test.result }}',
              'integration_status': '${{ needs.integration.result }}',
              'overall_success': '${{ needs.test.result }}' == 'success' and '${{ needs.integration.result }}' == 'success'
          }

          # Load or create CI history
          history_path = Path('ci_feedback_history.json')
          if history_path.exists():
              with open(history_path) as f:
                  history = json.load(f)
          else:
              history = {'runs': [], 'summary': {'total': 0, 'passed': 0}}

          # Append result
          history['runs'].append(ci_result)
          history['runs'] = history['runs'][-100:]  # Keep last 100 runs
          history['summary']['total'] += 1
          if ci_result['overall_success']:
              history['summary']['passed'] += 1

          # Calculate success rate for bandit feedback
          if history['summary']['total'] > 0:
              success_rate = history['summary']['passed'] / history['summary']['total']
              history['summary']['success_rate'] = round(success_rate, 4)

          # Save history
          with open(history_path, 'w') as f:
              json.dump(history, f, indent=2)

          print(f'CI Feedback recorded: {ci_result}')
          print(f'Overall success rate: {history[\"summary\"].get(\"success_rate\", 0):.2%}')
          "
        continue-on-error: true

      - name: Upload feedback data
        uses: actions/upload-artifact@v4
        with:
          name: ci-feedback
          path: ci_feedback_history.json
        continue-on-error: true

  # =================================================================
  # Summary Job
  # =================================================================
  summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [security, validate, quality, test, integration, feedback]
    if: always()

    steps:
      - name: Generate summary
        run: |
          echo "# CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Security | ${{ needs.security.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Validate | ${{ needs.validate.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Quality | ${{ needs.quality.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Tests | ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration | ${{ needs.integration.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Feedback | ${{ needs.feedback.result }} |" >> $GITHUB_STEP_SUMMARY

          # Overall status
          if [[ "${{ needs.test.result }}" == "success" && "${{ needs.validate.result }}" == "success" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ✅ Pipeline Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ❌ Pipeline Failed" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check overall status
        run: |
          if [[ "${{ needs.test.result }}" != "success" ]]; then
            echo "Tests failed"
            exit 1
          fi
          if [[ "${{ needs.validate.result }}" != "success" ]]; then
            echo "Validation failed"
            exit 1
          fi
          echo "All critical checks passed"
